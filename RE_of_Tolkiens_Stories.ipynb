{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9c9b0b",
   "metadata": {},
   "source": [
    "# Sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cde8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from itertools import combinations\n",
    "pattern = \"\"\"\n",
    "    NP: {<JJ>*<NN>+}   \n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\"\n",
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e9cead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.read_excel('name.xlsx')\n",
    "name['other_name'] = name['other_name'].fillna('')\n",
    "name['all_names'] = name['name'] +','+ name['other_name']\n",
    "name_dict = {}\n",
    "for i in range(len(name)):\n",
    "    m = name['all_names'][i].split(',')\n",
    "    race = name['race'][i]\n",
    "    for j in range(len(m)):\n",
    "        name_dict[m[j]] = [i,race]\n",
    "name_list=pd.DataFrame()\n",
    "name_list['code']=pd.DataFrame(list(name_dict.values()))[0]\n",
    "name_list['race']=pd.DataFrame(list(name_dict.values()))[1]\n",
    "name_list['name']=list(name_dict.keys())\n",
    "name_list = name_list.sort_values(by='race')\n",
    "name_list = name_list.sort_values(by='code')\n",
    "name_list.to_excel('name_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6ad6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list.columns=['name_code','race','name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9551ed9",
   "metadata": {},
   "source": [
    "### Extract the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69a44f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(text_name):\n",
    "    text = open(text_name,'r',encoding='utf-8').read()\n",
    "    text = text.replace('\\n',' ')   \n",
    "    text = text.replace('  ',' ')\n",
    "    tokenized_sentence = nltk.sent_tokenize(text)\n",
    "    tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]\n",
    "    tagged_words = [nltk.pos_tag(word) for word in tokenized_words]\n",
    "    word_tree = [chunker.parse(word) for word in tagged_words]\n",
    "    \n",
    "    return tokenized_sentence,word_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "746ef112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_with_relations_extraction(text):\n",
    "    text_data = pd.DataFrame(text)\n",
    "    text_data['times'] = 0 \n",
    "    text_data['names'] = '' \n",
    "    text_data.columns = ['sentence','times','names']\n",
    "    \n",
    "    for j  in range(len(text_data['sentence'])):\n",
    "        sentence = text_data['sentence'][j]\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for i in range (len(words)):\n",
    "            if (words[i] in list(name_list['name'])) and (words[i] not in text_data['names'][j]):\n",
    "                text_data['times'][j] = text_data['times'][j] + 1\n",
    "                if text_data['times'][j] == 1:\n",
    "                    text_data['names'][j] = []\n",
    "                    text_data['names'][j].append((words[i]))\n",
    "                elif text_data['times'][j] > 1:\n",
    "                    text_data['names'][j].append(words[i])\n",
    "    text_with_names = text_data.loc[text_data['times']>1]\n",
    "    text_with_names = text_with_names.reset_index(drop=True)\n",
    "    return(text_with_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf78561",
   "metadata": {},
   "source": [
    "### Compose the sentenses with the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07897547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_relation_extraction(text_way):\n",
    "    text,text_tree = data_clean(text_way)\n",
    "    sentences_with_relations_extraction_now = sentences_with_relations_extraction(text)\n",
    "    return(sentences_with_relations_extraction_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ab7acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_relations_extraction_Hobbit = text_relation_extraction('The Hobbit.txt')\n",
    "sentences_with_relations_extraction_Rings = text_relation_extraction('The Lord of the Rings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "310fcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_relations_extraction_Hobbit['book']='Hobbits'\n",
    "sentences_with_relations_extraction_Rings['book']='Ring'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cf867",
   "metadata": {},
   "source": [
    "# Create The Relationship Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cede2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations = pd.concat([sentences_with_relations_extraction_Hobbit,sentences_with_relations_extraction_Rings])\n",
    "r_h_relations = r_h_relations.reset_index(drop=True)\n",
    "r_h_relations['code'] = r_h_relations.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07a972b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations['names']=r_h_relations['names'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0ca1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "duple_relationship=pd.DataFrame()\n",
    "\n",
    "for i in range(len(r_h_relations)):\n",
    "    if i == 0:\n",
    "        a = pd.DataFrame((list(combinations(r_h_relations['names'][i],2))))\n",
    "        a['sentences_code'] = i \n",
    "        a['book']=r_h_relations['book'][i]\n",
    "        #a['relationship']=r_h_relations['relationship'][i]\n",
    "        duple_relationship = a\n",
    "        \n",
    "    else :\n",
    "        a = pd.DataFrame(duple_relationship)\n",
    "        b = pd.DataFrame((list(combinations(r_h_relations['names'][i],2))))\n",
    "        b['sentences_code'] = i \n",
    "        b['book']=r_h_relations['book'][i]\n",
    "#b['relationship']=r_h_relations['relationship'][i]\n",
    "        duple_relationship = pd.concat([a,b])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e55656cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>sentence_code</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Belladonna</td>\n",
       "      <td>0</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bungo</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>1</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Gandalf</td>\n",
       "      <td>2</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Gandalf</td>\n",
       "      <td>3</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dwalin</td>\n",
       "      <td>Balin</td>\n",
       "      <td>4</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pippin</td>\n",
       "      <td>Galadriel</td>\n",
       "      <td>2354</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sam</td>\n",
       "      <td>Galadriel</td>\n",
       "      <td>2354</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merry</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>2355</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merry</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>2356</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rose</td>\n",
       "      <td>Elanor</td>\n",
       "      <td>2357</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4342 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        c1          c2  sentence_code     book\n",
       "0    Bilbo  Belladonna              0  Hobbits\n",
       "0    Bungo       Bilbo              1  Hobbits\n",
       "0    Bilbo     Gandalf              2  Hobbits\n",
       "0    Bilbo     Gandalf              3  Hobbits\n",
       "0   Dwalin       Balin              4  Hobbits\n",
       "..     ...         ...            ...      ...\n",
       "8   Pippin   Galadriel           2354     Ring\n",
       "9      Sam   Galadriel           2354     Ring\n",
       "0    Merry      Pippin           2355     Ring\n",
       "0    Merry      Pippin           2356     Ring\n",
       "0     Rose      Elanor           2357     Ring\n",
       "\n",
       "[4342 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duple_relationship.columns=['c1','c2','sentence_code','book']\n",
    "duple_relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db3b02a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c2_name_code</th>\n",
       "      <th>c2_race</th>\n",
       "      <th>c1_name_code</th>\n",
       "      <th>c1_race</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>sentence_code</th>\n",
       "      <th>book</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>151</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Belladonna</td>\n",
       "      <td>0</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>151</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>186</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Bungo</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>1</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>395</td>\n",
       "      <td>Maia</td>\n",
       "      <td>151</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Gandalf</td>\n",
       "      <td>2</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>395</td>\n",
       "      <td>Maia</td>\n",
       "      <td>151</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Gandalf</td>\n",
       "      <td>3</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107</td>\n",
       "      <td>Dwarf</td>\n",
       "      <td>261</td>\n",
       "      <td>Dwarf</td>\n",
       "      <td>Dwalin</td>\n",
       "      <td>Balin</td>\n",
       "      <td>4</td>\n",
       "      <td>Hobbits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>387</td>\n",
       "      <td>Elf</td>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>Galadriel</td>\n",
       "      <td>2354</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>617</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Merry</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>2354</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>617</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Merry</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>2355</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>617</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Merry</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>2356</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4341</th>\n",
       "      <td>279</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>716</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Rose</td>\n",
       "      <td>Elanor</td>\n",
       "      <td>2357</td>\n",
       "      <td>Ring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4342 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      c2_name_code c2_race  c1_name_code c1_race      c1          c2  \\\n",
       "0              133  Hobbit           151  Hobbit   Bilbo  Belladonna   \n",
       "1              151  Hobbit           186  Hobbit   Bungo       Bilbo   \n",
       "2              395    Maia           151  Hobbit   Bilbo     Gandalf   \n",
       "3              395    Maia           151  Hobbit   Bilbo     Gandalf   \n",
       "4              107   Dwarf           261   Dwarf  Dwalin       Balin   \n",
       "...            ...     ...           ...     ...     ...         ...   \n",
       "4337           387     Elf           691  Hobbit  Pippin   Galadriel   \n",
       "4338           691  Hobbit           617  Hobbit   Merry      Pippin   \n",
       "4339           691  Hobbit           617  Hobbit   Merry      Pippin   \n",
       "4340           691  Hobbit           617  Hobbit   Merry      Pippin   \n",
       "4341           279  Hobbit           716  Hobbit    Rose      Elanor   \n",
       "\n",
       "      sentence_code     book  \n",
       "0                 0  Hobbits  \n",
       "1                 1  Hobbits  \n",
       "2                 2  Hobbits  \n",
       "3                 3  Hobbits  \n",
       "4                 4  Hobbits  \n",
       "...             ...      ...  \n",
       "4337           2354     Ring  \n",
       "4338           2354     Ring  \n",
       "4339           2355     Ring  \n",
       "4340           2356     Ring  \n",
       "4341           2357     Ring  \n",
       "\n",
       "[4342 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.merge(name_list,duple_relationship,left_on='name',right_on='c1')\n",
    "c1_c2 = pd.merge(name_list,a,left_on='name',right_on='c2')\n",
    "c1_c2.columns=['c2_name_code', 'c2_race', 'c2_name', 'c1_name_code', 'c1_race', 'c1_name',\n",
    "       'c1', 'c2', 'sentence_code','book']\n",
    "del c1_c2['c2_name']\n",
    "del c1_c2['c1_name']\n",
    "c1_c2 = c1_c2.sort_values(by='sentence_code')\n",
    "c1_c2 = c1_c2.reset_index(drop=True)\n",
    "c1_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48f38e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2['relationship'] = ''\n",
    "for i in range (len(c1_c2)):\n",
    "    #same one\n",
    "    if c1_c2['c1_name_code'][i] == c1_c2 ['c2_name_code'][i]:\n",
    "        c1_c2['relationship'][i] = 'same_people'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5476bd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     4342\n",
       "unique       2\n",
       "top           \n",
       "freq      4283\n",
       "Name: relationship, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_c2['relationship'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f4456",
   "metadata": {},
   "source": [
    "#### classify with the races ( the results have stored in the c1_c2_done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610da57",
   "metadata": {},
   "source": [
    "## create the relationship dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a45914bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from load_data import get_train_test_pd\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "from bert import modeling\n",
    "from albert_zh.extract_feature import BertVector\n",
    "\n",
    "#from att import Attention\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from torch.utils.data import _utils\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f012447",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations.to_excel('sentence_name.xlsx')\n",
    "c1_c2 = pd.read_excel('c1_c2_done.xlsx', index_col=0)\n",
    "sentence = pd.read_excel('sentence_name.xlsx',index_col=0)\n",
    "c1_c2=pd.merge(c1_c2,sentence,how ='left',left_on='sentence_code',right_on='code')\n",
    "del c1_c2['sentence_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a5c1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2.columns=['c2_name_code','c2_race','c1_name_code','c1_race','c1','c2','sentence_code','relationship','sentence','times','names','book','code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55988f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2 = c1_c2.sample(frac=1).reset_index(drop=True)\n",
    "c1_c2 = c1_c2.dropna()\n",
    "c1_c2 = c1_c2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9fefdb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c2_name_code</th>\n",
       "      <th>c2_race</th>\n",
       "      <th>c1_name_code</th>\n",
       "      <th>c1_race</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>sentence_code</th>\n",
       "      <th>relationship</th>\n",
       "      <th>sentence</th>\n",
       "      <th>times</th>\n",
       "      <th>names</th>\n",
       "      <th>book</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768</td>\n",
       "      <td>Man</td>\n",
       "      <td>770</td>\n",
       "      <td>Man</td>\n",
       "      <td>Théoden</td>\n",
       "      <td>Thengel</td>\n",
       "      <td>977</td>\n",
       "      <td>son</td>\n",
       "      <td>‘You may say this to Théoden son of Thengel: o...</td>\n",
       "      <td>3</td>\n",
       "      <td>['Théoden', 'Thengel', 'Sauron']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>Man</td>\n",
       "      <td>56</td>\n",
       "      <td>Man</td>\n",
       "      <td>Aragorn</td>\n",
       "      <td>Arathorn</td>\n",
       "      <td>1183</td>\n",
       "      <td>son</td>\n",
       "      <td>‘Hail Aragorn son of Arathorn!’ she said.</td>\n",
       "      <td>2</td>\n",
       "      <td>['Aragorn', 'Arathorn']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>1183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>540</td>\n",
       "      <td>Man</td>\n",
       "      <td>288</td>\n",
       "      <td>Man</td>\n",
       "      <td>Elendil</td>\n",
       "      <td>Isildur</td>\n",
       "      <td>568</td>\n",
       "      <td>parentage</td>\n",
       "      <td>‘Little do I resemble the figures of Elendil a...</td>\n",
       "      <td>3</td>\n",
       "      <td>['Elendil', 'Isildur', 'Denethor']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>169</td>\n",
       "      <td>Man</td>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>Boromir</td>\n",
       "      <td>1670</td>\n",
       "      <td>friend</td>\n",
       "      <td>Pippin saw his carven face with its proud bone...</td>\n",
       "      <td>3</td>\n",
       "      <td>['Pippin', 'Boromir', 'Aragorn']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>1670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>261</td>\n",
       "      <td>Dwarf</td>\n",
       "      <td>667</td>\n",
       "      <td>Dwarf</td>\n",
       "      <td>Ori</td>\n",
       "      <td>Dwalin</td>\n",
       "      <td>59</td>\n",
       "      <td>friend</td>\n",
       "      <td>After that Kili and Oin and Gloin and Don; nex...</td>\n",
       "      <td>6</td>\n",
       "      <td>['Ori', 'Nori', 'Bifur', 'Bofur', 'Dwalin', 'B...</td>\n",
       "      <td>Hobbits</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>380</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>617</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Merry</td>\n",
       "      <td>Frodo</td>\n",
       "      <td>288</td>\n",
       "      <td>friend</td>\n",
       "      <td>Merry was tying it up, and Pippin was already ...</td>\n",
       "      <td>4</td>\n",
       "      <td>['Merry', 'Pippin', 'Sam', 'Frodo']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>380</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>151</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Bilbo</td>\n",
       "      <td>Frodo</td>\n",
       "      <td>2339</td>\n",
       "      <td>uncle</td>\n",
       "      <td>Here Bilbo’s hand ended and Frodo had written:...</td>\n",
       "      <td>2</td>\n",
       "      <td>['Bilbo', 'Frodo']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>395</td>\n",
       "      <td>Maia</td>\n",
       "      <td>377</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Fatty</td>\n",
       "      <td>Gandalf</td>\n",
       "      <td>311</td>\n",
       "      <td>horse</td>\n",
       "      <td>But if Fatty is willing to hold the fort, and ...</td>\n",
       "      <td>2</td>\n",
       "      <td>['Fatty', 'Gandalf']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>560</td>\n",
       "      <td>Elf</td>\n",
       "      <td>691</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Pippin</td>\n",
       "      <td>Legolas</td>\n",
       "      <td>1790</td>\n",
       "      <td>friend</td>\n",
       "      <td>They have all gone to some doom: Gandalf and P...</td>\n",
       "      <td>7</td>\n",
       "      <td>['Gandalf', 'Pippin', 'Sam', 'Frodo', 'Strider...</td>\n",
       "      <td>Ring</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>734</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>380</td>\n",
       "      <td>Hobbit</td>\n",
       "      <td>Frodo</td>\n",
       "      <td>Sam</td>\n",
       "      <td>1461</td>\n",
       "      <td>friend</td>\n",
       "      <td>Frodo’s head was bowed over his knees, but Sam...</td>\n",
       "      <td>2</td>\n",
       "      <td>['Frodo', 'Sam']</td>\n",
       "      <td>Ring</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3136 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      c2_name_code c2_race  c1_name_code c1_race       c1        c2  \\\n",
       "0              768     Man           770     Man  Théoden   Thengel   \n",
       "1               69     Man            56     Man  Aragorn  Arathorn   \n",
       "2              540     Man           288     Man  Elendil   Isildur   \n",
       "3              169     Man           691  Hobbit   Pippin   Boromir   \n",
       "4              261   Dwarf           667   Dwarf      Ori    Dwalin   \n",
       "...            ...     ...           ...     ...      ...       ...   \n",
       "3131           380  Hobbit           617  Hobbit    Merry     Frodo   \n",
       "3132           380  Hobbit           151  Hobbit    Bilbo     Frodo   \n",
       "3133           395    Maia           377  Hobbit    Fatty   Gandalf   \n",
       "3134           560     Elf           691  Hobbit   Pippin   Legolas   \n",
       "3135           734  Hobbit           380  Hobbit    Frodo       Sam   \n",
       "\n",
       "      sentence_code relationship  \\\n",
       "0               977          son   \n",
       "1              1183          son   \n",
       "2               568    parentage   \n",
       "3              1670       friend   \n",
       "4                59       friend   \n",
       "...             ...          ...   \n",
       "3131            288       friend   \n",
       "3132           2339        uncle   \n",
       "3133            311        horse   \n",
       "3134           1790       friend   \n",
       "3135           1461       friend   \n",
       "\n",
       "                                               sentence  times  \\\n",
       "0     ‘You may say this to Théoden son of Thengel: o...      3   \n",
       "1             ‘Hail Aragorn son of Arathorn!’ she said.      2   \n",
       "2     ‘Little do I resemble the figures of Elendil a...      3   \n",
       "3     Pippin saw his carven face with its proud bone...      3   \n",
       "4     After that Kili and Oin and Gloin and Don; nex...      6   \n",
       "...                                                 ...    ...   \n",
       "3131  Merry was tying it up, and Pippin was already ...      4   \n",
       "3132  Here Bilbo’s hand ended and Frodo had written:...      2   \n",
       "3133  But if Fatty is willing to hold the fort, and ...      2   \n",
       "3134  They have all gone to some doom: Gandalf and P...      7   \n",
       "3135  Frodo’s head was bowed over his knees, but Sam...      2   \n",
       "\n",
       "                                                  names     book  code  \n",
       "0                      ['Théoden', 'Thengel', 'Sauron']     Ring   977  \n",
       "1                               ['Aragorn', 'Arathorn']     Ring  1183  \n",
       "2                    ['Elendil', 'Isildur', 'Denethor']     Ring   568  \n",
       "3                      ['Pippin', 'Boromir', 'Aragorn']     Ring  1670  \n",
       "4     ['Ori', 'Nori', 'Bifur', 'Bofur', 'Dwalin', 'B...  Hobbits    59  \n",
       "...                                                 ...      ...   ...  \n",
       "3131                ['Merry', 'Pippin', 'Sam', 'Frodo']     Ring   288  \n",
       "3132                                 ['Bilbo', 'Frodo']     Ring  2339  \n",
       "3133                               ['Fatty', 'Gandalf']     Ring   311  \n",
       "3134  ['Gandalf', 'Pippin', 'Sam', 'Frodo', 'Strider...     Ring  1790  \n",
       "3135                                   ['Frodo', 'Sam']     Ring  1461  \n",
       "\n",
       "[3136 rows x 13 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d40beb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import bert\n",
    "from transformers import BertTokenizer\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9659bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend          2047\n",
      "enermy           236\n",
      "horse            164\n",
      "cousin           156\n",
      "son               89\n",
      "spouse            64\n",
      "same_people       60\n",
      "uncle             51\n",
      "descendant        47\n",
      "ancestor          43\n",
      "nephew            41\n",
      "parentage         38\n",
      "sibling           35\n",
      "same_line         23\n",
      "other             23\n",
      "same_race          8\n",
      "chief              5\n",
      "hand               4\n",
      "unknown            1\n",
      "neighborhood       1\n",
      "Name: relationship, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = c1_c2\n",
    "relations = list(df['relationship'].unique())\n",
    "relations.remove('unknown')\n",
    "relation_dict = {'unknown': 0}\n",
    "relation_dict.update(dict(zip(relations, range(1, len(relations)+1))))\n",
    "\n",
    "with open('rel_dict.json', 'w', encoding='utf-8') as h:\n",
    "    h.write(json.dumps(relation_dict, ensure_ascii=False, indent=2))\n",
    "\n",
    "pprint(df['relationship'].value_counts())\n",
    "df['rel'] = df['relationship'].apply(lambda x: relation_dict[x])\n",
    "\n",
    "texts = []\n",
    "for per1, per2,r1,r2, text in zip(df['c1'].tolist(), df['c2'].tolist(),df['c1_race'].tolist(), df['c2_race'].tolist(), df['sentence'].tolist()):\n",
    "    text = '$ '.join([per1, per2,r1,r2, text])\n",
    "    texts.append(text)\n",
    "\n",
    "df['text'] = texts\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1024)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, rel in zip(train_df['text'].tolist(), train_df['rel'].tolist()):\n",
    "        f.write(str(rel)+' '+text+'\\n')\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as g:\n",
    "    for text, rel in zip(test_df['text'].tolist(), test_df['rel'].tolist()):\n",
    "        g.write(str(rel)+' '+text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3da2b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friend          2047\n",
      "enermy           236\n",
      "horse            164\n",
      "cousin           156\n",
      "son               89\n",
      "spouse            64\n",
      "same_people       60\n",
      "uncle             51\n",
      "descendant        47\n",
      "ancestor          43\n",
      "nephew            41\n",
      "parentage         38\n",
      "sibling           35\n",
      "same_line         23\n",
      "other             23\n",
      "same_race          8\n",
      "chief              5\n",
      "hand               4\n",
      "unknown            1\n",
      "neighborhood       1\n",
      "Name: relationship, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = c1_c2\n",
    "relations = list(df['relationship'].unique())\n",
    "relations.remove('unknown')\n",
    "relation_dict = {'unknown': 0}\n",
    "relation_dict.update(dict(zip(relations, range(1, len(relations)+1))))\n",
    "\n",
    "with open('rel_dict.json', 'w', encoding='utf-8') as h:\n",
    "    h.write(json.dumps(relation_dict, ensure_ascii=False, indent=2))\n",
    "\n",
    "pprint(df['relationship'].value_counts())\n",
    "df['rel'] = df['relationship'].apply(lambda x: relation_dict[x])\n",
    "\n",
    "texts = []\n",
    "for per1, per2,r1,r2, text in zip(df['c1'].tolist(), df['c2'].tolist(),df['c1_race'].tolist(), df['c2_race'].tolist(), df['sentence'].tolist()):\n",
    "    text = '$ '.join([per1, per2,r1,r2, text])\n",
    "    texts.append(text)\n",
    "\n",
    "df['text'] = texts\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1024)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, rel in zip(train_df['text'].tolist(), train_df['rel'].tolist()):\n",
    "        f.write(str(rel)+' '+text+'\\n')\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as g:\n",
    "    for text, rel in zip(test_df['text'].tolist(), test_df['rel'].tolist()):\n",
    "        g.write(str(rel)+' '+text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b3cfe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0     5  Isildur$ Sauron$ Man$ Maia$ For this war will ...\n",
      "1     9  Dori$ Ori$ Dwarf$ Dwarf$ Bilbo was running rou...\n",
      "2     3  Sam$ Frodo$ Hobbit$ Hobbit$ Sam’s guess was th...\n",
      "3     5  Bilbo$ Gollum$ Hobbit$ Hobbit$ So Bilbo decide...\n",
      "4     3  Bombur$ Dwalin$ Dwarf$ Dwarf$ Kili and Fili br...\n",
      "  label                                               text\n",
      "0     5  Sauron$ Isildur$ Maia$ Man$ I beheld the last ...\n",
      "1     5  Sam$ Shagrat$ Hobbit$ Orc$ Sam staggered, and ...\n",
      "2     3  Nori$ Bifur$ Dwarf$ Dwarf$ After that Kili and...\n",
      "3     3  Éomer$ Aragorn$ Man$ Man$ It has been knife-wo...\n",
      "4     5  Sauron$ Lúthien$ Maia$ Elf$ But nobler is his ...\n",
      "          text_len\n",
      "count  2509.000000\n",
      "mean     58.977282\n",
      "std      25.987696\n",
      "min      15.000000\n",
      "25%      41.000000\n",
      "50%      54.000000\n",
      "75%      72.000000\n",
      "max     209.000000\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = [_.strip() for _ in f.readlines()]\n",
    "\n",
    "    labels, texts = [], []\n",
    "    for line in content:\n",
    "        parts = line.split()\n",
    "        label, text = parts[0], ' '.join(parts[1:])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "\n",
    "    return labels, texts\n",
    "\n",
    "#\n",
    "def get_train_test_pd():\n",
    "    file_path = 'train.txt'\n",
    "    labels, texts = read_txt_file(file_path)\n",
    "    train_df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "    file_path = 'test.txt'\n",
    "    labels, texts = read_txt_file(file_path)\n",
    "    test_df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_df, test_df = get_train_test_pd()\n",
    "    print(train_df.head())\n",
    "    print(test_df.head())\n",
    "\n",
    "    train_df['text_len'] = train_df['text'].apply(lambda x: len(tz.tokenize(x)))\n",
    "    print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46651e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_padding(data, max_len): \n",
    "    if len(data) < max_len:\n",
    "        pad_len = max_len - len(data)\n",
    "        padding = [0 for _ in range(pad_len)]\n",
    "        data = torch.tensor(data + padding )\n",
    "    else:\n",
    "        data = torch.tensor(data [:mapax_len])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9f6173f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bc9a4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_excel('train_df.xlsx')\n",
    "test_df.to_excel('test_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb6421",
   "metadata": {},
   "source": [
    "## DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85a8c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd8df7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_excel('train_df.xlsx',index_col=0)\n",
    "test_df = pd.read_excel('test_df.xlsx',index_col=0)\n",
    "train_df = all_data.iloc[:1882,:]\n",
    "eval_df =  all_data.iloc[1882:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ba0fd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel('validation_set.xlsx')\n",
    "train_df.to_excel('train_set.xlsx')\n",
    "eval_df.to_excel('test_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3ce8609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)\n",
    "eval_data = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ae7cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5f2a28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a0999a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00792d3c4c94e1686dd7106325800dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd86c9db1a604aa792fb99344f99ad9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc65749ac046de8d3bcab2c733e6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_eval = eval_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ee9984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "94245dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4890377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39955b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text, text_len, __index_level_0__. If text, text_len, __index_level_0__ are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1882\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='2360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/2360 00:55 < 7:14:09, 0.09 it/s, Epoch 0.05/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16880/3355044788.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1399\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2002\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "train_out = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c73cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e93a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "prediction = classifier(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=[]\n",
    "for i in range(len(prediction)):\n",
    "    pred_label.append(int(prediction[i]['label'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "metrics.f1_score(pred_label, test_data['label'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf67df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "for i in range(len(prediction)):\n",
    "    if test_data['label'][i] == pred_label[i]:\n",
    "        a=a+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43185773",
   "metadata": {},
   "source": [
    "## LSTM and TextCNN with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "53940a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Activation, Input\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D\n",
    "from keras.layers import  BatchNormalization\n",
    "from keras.layers import Convolution1D, Conv1D,MaxPooling1D\n",
    "from keras.layers import Dense, Embedding, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
    "#from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba as jb\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras .models import  Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import  Conv1D\n",
    "from keras.layers.convolutional import  MaxPooling1D\n",
    "import keras\n",
    "from keras .models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk import word_tokenize     \n",
    "import gensim\n",
    "from keras.layers import Input\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cf84f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"train.txt\",encoding='utf-8')\n",
    "sentences = f.readlines() \n",
    "f.close()\n",
    "w2v_model =Word2Vec(sentences, hs=1, min_count=1, window=5, vector_size=500)\n",
    "w2v_model.wv.save_word2vec_format(\"./word2Vec\" + \".pkl\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8608bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train_set.xlsx',index_col=0)\n",
    "test_df = pd.read_excel('test_set.xlsx',index_col=0)\n",
    "eval_df = pd.read_excel('validation_set.xlsx',index_col=0)\n",
    "all_train = pd.concat([train_df,eval_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5550d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=encode_docs(tokenizer,512,all_train['text'])\n",
    "y_train=all_train['label']\n",
    "y_train=np.array(y_train)#(130583, 1)\n",
    "Y_train=keras.utils.np_utils.to_categorical(y_train,20)#\n",
    "X_test=encode_docs(tokenizer,512,test_df['text'])\n",
    "y_test=test_df['label']\n",
    "y_test=np.array(y_test)#(130583, 1)\n",
    "Y_test=keras.utils.np_utils.to_categorical(y_test,20)#\n",
    "vocad_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5697de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d9046",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "36289bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCNN_model_1(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n",
    "    main_input = Input(shape=(512,), dtype='float64')\n",
    "    \n",
    "    embedder = (Embedding(vocad_size, 100, input_length=50))\n",
    "    embed = embedder(main_input)\n",
    "    \n",
    "    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn1 = MaxPooling1D(pool_size=49)(cnn1)\n",
    "    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn2 = MaxPooling1D(pool_size=48)(cnn2)\n",
    "    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn3 = MaxPooling1D(pool_size=47)(cnn3)\n",
    "   \n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(20, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    one_hot_labels =one_hot_labels = keras.utils.np_utils.to_categorical(y_train,20) \n",
    "    model.fit(x_train_padded_seqs, one_hot_labels, batch_size=800, epochs=20)\n",
    "    #y_test_onehot = keras.utils.to_categorical(y_test, num_classes=3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38522a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab) + 1, 500))\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    " \n",
    "\n",
    "def TextCNN_model_2(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test,embedding_matrix):\n",
    "    \n",
    "    main_input = Input(shape=(512,), dtype='float64')\n",
    "    \n",
    "    embedder = Embedding(len(vocab) + 1, 500, input_length=512, weights=[embedding_matrix], trainable=False)\n",
    "    #embedder = Embedding(len(vocab) + 1, 300, input_length=50, trainable=False)\n",
    "    embed = embedder(main_input)\n",
    "    \n",
    "    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn1 = MaxPooling1D(pool_size=49)(cnn1)\n",
    "    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn2 = MaxPooling1D(pool_size=48)(cnn2)\n",
    "    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn3 = MaxPooling1D(pool_size=47)(cnn3)\n",
    "    \n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(20, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    one_hot_labels = keras.utils.np_utils.to_categorical(y_train,20) \n",
    "    model.fit(x_train_padded_seqs, one_hot_labels, batch_size=800, epochs=20)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66a0b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef848923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16880/3045490482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextcnnw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextCNN_model_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16880/428811253.py\u001b[0m in \u001b[0;36mTextCNN_model_2\u001b[1;34m(x_train_padded_seqs, y_train, x_test_padded_seqs, y_test, embedding_matrix)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mone_hot_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_padded_seqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3128\u001b[0m       (graph_function,\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1959\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "textcnnw2v = TextCNN_model_2(X_train, y_train, X_test, y_test,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ca529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## textcnnw2v\n",
    "y_predict = textcnnw2v.predict(X_test) \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn = TextCNN_model_1(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdeed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textcnn\n",
    "y_predict = textcnn.predict(X_test)  \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32567045",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593821d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_dim = 500 \n",
    "maxlen = 512 \n",
    "batch_size = 100 \n",
    "n_epoch = 20   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b74eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=encode_docs(tokenizer,512,train_df['text'])\n",
    "y_train=train_df['label']\n",
    "y_train=np.array(y_train)#(130583, 1)\n",
    "Y_train=keras.utils.np_utils.to_categorical(y_train,20)#\n",
    "\n",
    "X_eval=encode_docs(tokenizer,512,eval_df['text'])\n",
    "y_eval=eval_df['label']\n",
    "y_eval=np.array(y_eval)#(130583, 1)\n",
    "Y_eval=keras.utils.np_utils.to_categorical(y_eval,20)#\n",
    "#vocad_size=len(tokenizer.word_index)+1\n",
    "\n",
    "X_test=encode_docs(tokenizer,512,test_df['text'])\n",
    "y_test=test_df['label']\n",
    "y_test=np.array(y_test)#(130583, 1)\n",
    "Y_test=keras.utils.np_utils.to_categorical(y_test,20)#\n",
    "vocad_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b02029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(p_n_symbols, p_embedding_weights, p_X_train, p_y_train, p_X_eval, p_y_eval, p_X_test, p_y_test\n",
    "):\n",
    "    print('创建模型...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=vocab_dim,  \n",
    "                        input_dim=p_n_symbols, \n",
    "                        mask_zero=True,         \n",
    "                        weights=[p_embedding_weights],   \n",
    "                        input_length=maxlen ))      \n",
    "    \n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2 ))\n",
    "    model.add(Dense(units=20,  \n",
    "                    activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    print('modeling...')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"training...\")\n",
    "    train_label_one_hot = keras.utils.np_utils.to_categorical(p_y_train,20)  \n",
    "    test_label_one_hot = keras.utils.np_utils.to_categorical(p_y_test,20)  \n",
    "    eval_label_one_hot = keras.utils.np_utils.to_categorical(p_y_eval,20) \n",
    "    \n",
    "    train_history = model.fit(p_X_train, train_label_one_hot, batch_size=batch_size, epochs=n_epoch,\n",
    "              validation_data=(p_X_eval, eval_label_one_hot))\n",
    "    return train_history,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "his_lstm,lstm =  train_lstm(4534,embedding_matrix,X_train, y_train, X_eval, y_eval, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6494ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## textcnn\n",
    "y_predict = lstm.predict(X_test) \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuray', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
