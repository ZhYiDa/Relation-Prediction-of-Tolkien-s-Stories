{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d9c9b0b",
   "metadata": {},
   "source": [
    "# Sentence extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e0001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from itertools import combinations\n",
    "pattern = \"\"\"\n",
    "    NP: {<JJ>*<NN>+}   \n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\"\n",
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9cead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = pd.read_excel('name.xlsx')\n",
    "name['other_name'] = name['other_name'].fillna('')\n",
    "name['all_names'] = name['name'] +','+ name['other_name']\n",
    "name_dict = {}\n",
    "for i in range(len(name)):\n",
    "    m = name['all_names'][i].split(',')\n",
    "    race = name['race'][i]\n",
    "    for j in range(len(m)):\n",
    "        name_dict[m[j]] = [i,race]\n",
    "name_list=pd.DataFrame()\n",
    "name_list['code']=pd.DataFrame(list(name_dict.values()))[0]\n",
    "name_list['race']=pd.DataFrame(list(name_dict.values()))[1]\n",
    "name_list['name']=list(name_dict.keys())\n",
    "name_list = name_list.sort_values(by='race')\n",
    "name_list = name_list.sort_values(by='code')\n",
    "name_list.to_excel('name_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list.columns=['name_code','race','name']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9551ed9",
   "metadata": {},
   "source": [
    "### Extract the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a44f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(text_name):\n",
    "    text = open(text_name,'r',encoding='utf-8').read()\n",
    "    text = text.replace('\\n',' ')   \n",
    "    text = text.replace('  ',' ')\n",
    "    tokenized_sentence = nltk.sent_tokenize(text)\n",
    "    tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]\n",
    "    tagged_words = [nltk.pos_tag(word) for word in tokenized_words]\n",
    "    word_tree = [chunker.parse(word) for word in tagged_words]\n",
    "    \n",
    "    return tokenized_sentence,word_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746ef112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_with_relations_extraction(text):\n",
    "    text_data = pd.DataFrame(text)\n",
    "    text_data['times'] = 0 \n",
    "    text_data['names'] = '' \n",
    "    text_data.columns = ['sentence','times','names']\n",
    "    \n",
    "    for j  in range(len(text_data['sentence'])):\n",
    "        sentence = text_data['sentence'][j]\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        for i in range (len(words)):\n",
    "            if (words[i] in list(name_list['name'])) and (words[i] not in text_data['names'][j]):\n",
    "                text_data['times'][j] = text_data['times'][j] + 1\n",
    "                if text_data['times'][j] == 1:\n",
    "                    text_data['names'][j] = []\n",
    "                    text_data['names'][j].append((words[i]))\n",
    "                elif text_data['times'][j] > 1:\n",
    "                    text_data['names'][j].append(words[i])\n",
    "    text_with_names = text_data.loc[text_data['times']>1]\n",
    "    text_with_names = text_with_names.reset_index(drop=True)\n",
    "    return(text_with_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf78561",
   "metadata": {},
   "source": [
    "### Compose the sentenses with the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_relation_extraction(text_way):\n",
    "    text,text_tree = data_clean(text_way)\n",
    "    sentences_with_relations_extraction_now = sentences_with_relations_extraction(text)\n",
    "    return(sentences_with_relations_extraction_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab7acca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_relations_extraction_Hobbit = text_relation_extraction('The Hobbit.txt')\n",
    "sentences_with_relations_extraction_Rings = text_relation_extraction('The Lord of the Rings.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310fcb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_relations_extraction_Hobbit['book']='Hobbits'\n",
    "sentences_with_relations_extraction_Rings['book']='Ring'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79cf867",
   "metadata": {},
   "source": [
    "# Create The Relationship Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations = pd.concat([sentences_with_relations_extraction_Hobbit,sentences_with_relations_extraction_Rings])\n",
    "r_h_relations = r_h_relations.reset_index(drop=True)\n",
    "r_h_relations['code'] = r_h_relations.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a972b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations['names']=r_h_relations['names'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "duple_relationship=pd.DataFrame()\n",
    "\n",
    "for i in range(len(r_h_relations)):\n",
    "    if i == 0:\n",
    "        a = pd.DataFrame((list(combinations(r_h_relations['names'][i],2))))\n",
    "        a['sentences_code'] = i \n",
    "        a['book']=r_h_relations['book'][i]\n",
    "        #a['relationship']=r_h_relations['relationship'][i]\n",
    "        duple_relationship = a\n",
    "        \n",
    "    else :\n",
    "        a = pd.DataFrame(duple_relationship)\n",
    "        b = pd.DataFrame((list(combinations(r_h_relations['names'][i],2))))\n",
    "        b['sentences_code'] = i \n",
    "        b['book']=r_h_relations['book'][i]\n",
    "#b['relationship']=r_h_relations['relationship'][i]\n",
    "        duple_relationship = pd.concat([a,b])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55656cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "duple_relationship.columns=['c1','c2','sentence_code','book']\n",
    "duple_relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b02a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.merge(name_list,duple_relationship,left_on='name',right_on='c1')\n",
    "c1_c2 = pd.merge(name_list,a,left_on='name',right_on='c2')\n",
    "c1_c2.columns=['c2_name_code', 'c2_race', 'c2_name', 'c1_name_code', 'c1_race', 'c1_name',\n",
    "       'c1', 'c2', 'sentence_code','book']\n",
    "del c1_c2['c2_name']\n",
    "del c1_c2['c1_name']\n",
    "c1_c2 = c1_c2.sort_values(by='sentence_code')\n",
    "c1_c2 = c1_c2.reset_index(drop=True)\n",
    "c1_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f38e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2['relationship'] = ''\n",
    "for i in range (len(c1_c2)):\n",
    "    #same one\n",
    "    if c1_c2['c1_name_code'][i] == c1_c2 ['c2_name_code'][i]:\n",
    "        c1_c2['relationship'][i] = 'same_people'\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2['relationship'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f4456",
   "metadata": {},
   "source": [
    "#### classify with the races ( the results have stored in the c1_c2_done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610da57",
   "metadata": {},
   "source": [
    "## create the relationship dataset for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45914bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from load_data import get_train_test_pd\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "from bert import modeling\n",
    "from albert_zh.extract_feature import BertVector\n",
    "\n",
    "#from att import Attention\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "from torch.utils.data import _utils\n",
    "import warnings\n",
    "warnings. filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f012447",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_h_relations.to_excel('sentence_name.xlsx')\n",
    "c1_c2 = pd.read_excel('c1_c2_done.xlsx', index_col=0)\n",
    "sentence = pd.read_excel('sentence_name.xlsx',index_col=0)\n",
    "c1_c2=pd.merge(c1_c2,sentence,how ='left',left_on='sentence_code',right_on='code')\n",
    "del c1_c2['sentence_x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2.columns=['c2_name_code','c2_race','c1_name_code','c1_race','c1','c2','sentence_code','relationship','sentence','times','names','book','code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55988f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2 = c1_c2.sample(frac=1).reset_index(drop=True)\n",
    "c1_c2 = c1_c2.dropna()\n",
    "c1_c2 = c1_c2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40beb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import bert\n",
    "from transformers import BertTokenizer\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9659bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = c1_c2\n",
    "relations = list(df['relationship'].unique())\n",
    "relations.remove('unknown')\n",
    "relation_dict = {'unknown': 0}\n",
    "relation_dict.update(dict(zip(relations, range(1, len(relations)+1))))\n",
    "\n",
    "with open('rel_dict.json', 'w', encoding='utf-8') as h:\n",
    "    h.write(json.dumps(relation_dict, ensure_ascii=False, indent=2))\n",
    "\n",
    "pprint(df['relationship'].value_counts())\n",
    "df['rel'] = df['relationship'].apply(lambda x: relation_dict[x])\n",
    "\n",
    "texts = []\n",
    "for per1, per2,r1,r2, text in zip(df['c1'].tolist(), df['c2'].tolist(),df['c1_race'].tolist(), df['c2_race'].tolist(), df['sentence'].tolist()):\n",
    "    text = '$ '.join([per1, per2,r1,r2, text])\n",
    "    texts.append(text)\n",
    "\n",
    "df['text'] = texts\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1024)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, rel in zip(train_df['text'].tolist(), train_df['rel'].tolist()):\n",
    "        f.write(str(rel)+' '+text+'\\n')\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as g:\n",
    "    for text, rel in zip(test_df['text'].tolist(), test_df['rel'].tolist()):\n",
    "        g.write(str(rel)+' '+text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da2b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = c1_c2\n",
    "relations = list(df['relationship'].unique())\n",
    "relations.remove('unknown')\n",
    "relation_dict = {'unknown': 0}\n",
    "relation_dict.update(dict(zip(relations, range(1, len(relations)+1))))\n",
    "\n",
    "with open('rel_dict.json', 'w', encoding='utf-8') as h:\n",
    "    h.write(json.dumps(relation_dict, ensure_ascii=False, indent=2))\n",
    "\n",
    "pprint(df['relationship'].value_counts())\n",
    "df['rel'] = df['relationship'].apply(lambda x: relation_dict[x])\n",
    "\n",
    "texts = []\n",
    "for per1, per2,r1,r2, text in zip(df['c1'].tolist(), df['c2'].tolist(),df['c1_race'].tolist(), df['c2_race'].tolist(), df['sentence'].tolist()):\n",
    "    text = '$ '.join([per1, per2,r1,r2, text])\n",
    "    texts.append(text)\n",
    "\n",
    "df['text'] = texts\n",
    "\n",
    "train_df = df.sample(frac=0.8, random_state=1024)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    for text, rel in zip(train_df['text'].tolist(), train_df['rel'].tolist()):\n",
    "        f.write(str(rel)+' '+text+'\\n')\n",
    "\n",
    "with open('test.txt', 'w', encoding='utf-8') as g:\n",
    "    for text, rel in zip(test_df['text'].tolist(), test_df['rel'].tolist()):\n",
    "        g.write(str(rel)+' '+text+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cfe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "\n",
    "# \n",
    "def read_txt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = [_.strip() for _ in f.readlines()]\n",
    "\n",
    "    labels, texts = [], []\n",
    "    for line in content:\n",
    "        parts = line.split()\n",
    "        label, text = parts[0], ' '.join(parts[1:])\n",
    "        labels.append(label)\n",
    "        texts.append(text)\n",
    "\n",
    "    return labels, texts\n",
    "\n",
    "#\n",
    "def get_train_test_pd():\n",
    "    file_path = 'train.txt'\n",
    "    labels, texts = read_txt_file(file_path)\n",
    "    train_df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "    file_path = 'test.txt'\n",
    "    labels, texts = read_txt_file(file_path)\n",
    "    test_df = pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_df, test_df = get_train_test_pd()\n",
    "    print(train_df.head())\n",
    "    print(test_df.head())\n",
    "\n",
    "    train_df['text_len'] = train_df['text'].apply(lambda x: len(tz.tokenize(x)))\n",
    "    print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46651e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_padding(data, max_len): \n",
    "    if len(data) < max_len:\n",
    "        pad_len = max_len - len(data)\n",
    "        padding = [0 for _ in range(pad_len)]\n",
    "        data = torch.tensor(data + padding )\n",
    "    else:\n",
    "        data = torch.tensor(data [:mapax_len])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6173f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a4f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_excel('train_df.xlsx')\n",
    "test_df.to_excel('test_df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eb6421",
   "metadata": {},
   "source": [
    "## DistilBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8df7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_excel('train_df.xlsx',index_col=0)\n",
    "test_df = pd.read_excel('test_df.xlsx',index_col=0)\n",
    "train_df = all_data.iloc[:1882,:]\n",
    "eval_df =  all_data.iloc[1882:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0fd2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel('validation_set.xlsx')\n",
    "train_df.to_excel('train_set.xlsx')\n",
    "eval_df.to_excel('test_set.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_pandas(train_df)\n",
    "test_data = Dataset.from_pandas(test_df)\n",
    "eval_data = Dataset.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2a28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True,padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0999a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_eval = eval_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94245dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds.predictions, eval_preds.label_ids\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39955b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "   \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    "\n",
    ")\n",
    "\n",
    "train_out = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73cb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e93a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "prediction = classifier(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=[]\n",
    "for i in range(len(prediction)):\n",
    "    pred_label.append(int(prediction[i]['label'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "metrics.f1_score(pred_label, test_data['label'], average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf67df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "for i in range(len(prediction)):\n",
    "    if test_data['label'][i] == pred_label[i]:\n",
    "        a=a+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43185773",
   "metadata": {},
   "source": [
    "## LSTM and TextCNN with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53940a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Activation, Input\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D\n",
    "from keras.layers import  BatchNormalization\n",
    "from keras.layers import Convolution1D, Conv1D,MaxPooling1D\n",
    "from keras.layers import Dense, Embedding, Input, Lambda, Reshape\n",
    "from keras.layers import Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, GRU, TimeDistributed, Bidirectional\n",
    "#from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba as jb\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import  pad_sequences\n",
    "from keras .models import  Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import  Conv1D\n",
    "from keras.layers.convolutional import  MaxPooling1D\n",
    "import keras\n",
    "from keras .models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk import word_tokenize     \n",
    "import gensim\n",
    "from keras.layers import Input\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf84f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"train.txt\",encoding='utf-8')\n",
    "sentences = f.readlines() \n",
    "f.close()\n",
    "w2v_model =Word2Vec(sentences, hs=1, min_count=1, window=5, vector_size=500)\n",
    "w2v_model.wv.save_word2vec_format(\"./word2Vec\" + \".pkl\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train_set.xlsx',index_col=0)\n",
    "test_df = pd.read_excel('test_set.xlsx',index_col=0)\n",
    "eval_df = pd.read_excel('validation_set.xlsx',index_col=0)\n",
    "all_train = pd.concat([train_df,eval_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5550d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=encode_docs(tokenizer,512,all_train['text'])\n",
    "y_train=all_train['label']\n",
    "y_train=np.array(y_train)#(130583, 1)\n",
    "Y_train=keras.utils.np_utils.to_categorical(y_train,20)#\n",
    "X_test=encode_docs(tokenizer,512,test_df['text'])\n",
    "y_test=test_df['label']\n",
    "y_test=np.array(y_test)#(130583, 1)\n",
    "Y_test=keras.utils.np_utils.to_categorical(y_test,20)#\n",
    "vocad_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d9046",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36289bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextCNN_model_1(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test):\n",
    "    main_input = Input(shape=(512,), dtype='float64')\n",
    "    \n",
    "    embedder = (Embedding(vocad_size, 100, input_length=50))\n",
    "    embed = embedder(main_input)\n",
    "    \n",
    "    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn1 = MaxPooling1D(pool_size=49)(cnn1)\n",
    "    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn2 = MaxPooling1D(pool_size=48)(cnn2)\n",
    "    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn3 = MaxPooling1D(pool_size=47)(cnn3)\n",
    "   \n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(20, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    one_hot_labels =one_hot_labels = keras.utils.np_utils.to_categorical(y_train,20) \n",
    "    model.fit(x_train_padded_seqs, one_hot_labels, batch_size=800, epochs=20)\n",
    "    #y_test_onehot = keras.utils.to_categorical(y_test, num_classes=3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38522a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab) + 1, 500))\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue\n",
    " \n",
    "\n",
    "def TextCNN_model_2(x_train_padded_seqs,y_train,x_test_padded_seqs,y_test,embedding_matrix):\n",
    "    \n",
    "    main_input = Input(shape=(512,), dtype='float64')\n",
    "    \n",
    "    embedder = Embedding(len(vocab) + 1, 500, input_length=512, weights=[embedding_matrix], trainable=False)\n",
    "    #embedder = Embedding(len(vocab) + 1, 300, input_length=50, trainable=False)\n",
    "    embed = embedder(main_input)\n",
    "    \n",
    "    cnn1 = Conv1D(256, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn1 = MaxPooling1D(pool_size=49)(cnn1)\n",
    "    cnn2 = Conv1D(256, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn2 = MaxPooling1D(pool_size=48)(cnn2)\n",
    "    cnn3 = Conv1D(256, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "    cnn3 = MaxPooling1D(pool_size=47)(cnn3)\n",
    "    \n",
    "    cnn = concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = Flatten()(cnn)\n",
    "    drop = Dropout(0.2)(flat)\n",
    "    main_output = Dense(20, activation='softmax')(drop)\n",
    "    model = Model(inputs=main_input, outputs=main_output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "    one_hot_labels = keras.utils.np_utils.to_categorical(y_train,20) \n",
    "    model.fit(x_train_padded_seqs, one_hot_labels, batch_size=800, epochs=20)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_docs(tokenizer,max_length,docs):\n",
    "    encoded=tokenizer.texts_to_sequences(docs)\n",
    "    padded=pad_sequences(encoded,maxlen=max_length,padding='post')\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef848923",
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnnw2v = TextCNN_model_2(X_train, y_train, X_test, y_test,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ca529",
   "metadata": {},
   "outputs": [],
   "source": [
    "## textcnnw2v\n",
    "y_predict = textcnnw2v.predict(X_test) \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbed9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "textcnn = TextCNN_model_1(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdeed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# textcnn\n",
    "y_predict = textcnn.predict(X_test)  \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuracy', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32567045",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593821d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_dim = 500 \n",
    "maxlen = 512 \n",
    "batch_size = 100 \n",
    "n_epoch = 20   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b74eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=encode_docs(tokenizer,512,train_df['text'])\n",
    "y_train=train_df['label']\n",
    "y_train=np.array(y_train)#(130583, 1)\n",
    "Y_train=keras.utils.np_utils.to_categorical(y_train,20)#\n",
    "\n",
    "X_eval=encode_docs(tokenizer,512,eval_df['text'])\n",
    "y_eval=eval_df['label']\n",
    "y_eval=np.array(y_eval)#(130583, 1)\n",
    "Y_eval=keras.utils.np_utils.to_categorical(y_eval,20)#\n",
    "#vocad_size=len(tokenizer.word_index)+1\n",
    "\n",
    "X_test=encode_docs(tokenizer,512,test_df['text'])\n",
    "y_test=test_df['label']\n",
    "y_test=np.array(y_test)#(130583, 1)\n",
    "Y_test=keras.utils.np_utils.to_categorical(y_test,20)#\n",
    "vocad_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b02029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(p_n_symbols, p_embedding_weights, p_X_train, p_y_train, p_X_eval, p_y_eval, p_X_test, p_y_test\n",
    "):\n",
    "    print('创建模型...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(output_dim=vocab_dim,  \n",
    "                        input_dim=p_n_symbols, \n",
    "                        mask_zero=True,         \n",
    "                        weights=[p_embedding_weights],   \n",
    "                        input_length=maxlen ))      \n",
    "    \n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2 ))\n",
    "    model.add(Dense(units=20,  \n",
    "                    activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    print('modeling...')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(\"training...\")\n",
    "    train_label_one_hot = keras.utils.np_utils.to_categorical(p_y_train,20)  \n",
    "    test_label_one_hot = keras.utils.np_utils.to_categorical(p_y_test,20)  \n",
    "    eval_label_one_hot = keras.utils.np_utils.to_categorical(p_y_eval,20) \n",
    "    \n",
    "    train_history = model.fit(p_X_train, train_label_one_hot, batch_size=batch_size, epochs=n_epoch,\n",
    "              validation_data=(p_X_eval, eval_label_one_hot))\n",
    "    return train_history,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b1706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "his_lstm,lstm =  train_lstm(4534,embedding_matrix,X_train, y_train, X_eval, y_eval, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6494ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## textcnn\n",
    "y_predict = lstm.predict(X_test) \n",
    "y_predict = np.argmax(y_predict,axis=1)\n",
    "test_y=np.array(y_test)\n",
    "print('accuray', metrics.accuracy_score(y_test, y_predict))\n",
    "print('f1-score:', metrics.f1_score(y_test, y_predict, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119dc25",
   "metadata": {},
   "source": [
    "## Visualization with neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ff207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import py2neo\n",
    "from py2neo import Graph,Node,Relationship,NodeMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('neo4j.xlsx',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c657c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['c2_name_code']\n",
    "del data['c1_name_code']\n",
    "del data['sentence_code']\n",
    "del data['sentence']\n",
    "data=data.reset_index(drop=1)\n",
    "data = data.drop_duplicates()\n",
    "data=data.reset_index(drop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb19f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_b=[]\n",
    "r_b=[]\n",
    "for i in range (len(data)):\n",
    "    if data['book'][i]=='Ring':\n",
    "        r_b.append(data['c1'][i])\n",
    "        r_b.append(data['c2'][i])\n",
    "    else:\n",
    "        h_b.append(data['c1'][i])\n",
    "        h_b.append(data['c2'][i])\n",
    "both_list = ['Bofur','Dwalin','Smaug','Thorin',     \n",
    "'Fundin','Bombur','Dori','Bill','Bilbo','Bifur','Gandalf','Tom'     ,   'Beorn'    ,  'Bard'      , \n",
    "'Balin'      ,'Elrond'    ,'Ori'        ,'Gollum'     ]\n",
    "only_h_list=list(set(h_b)-set(both_list))\n",
    "only_r_list=list(set(r_b)-set(both_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['c1_book']=''\n",
    "data['c2_book']=''\n",
    "for i in range(len(data)):\n",
    "    if data['c1'][i] in both_list:\n",
    "        data['c1_book'][i] = 'both'\n",
    "       # print(1)\n",
    "    elif data['c1'][i] in only_h_list:\n",
    "        data['c1_book'][i] = 'only_hobbits'\n",
    "       # print(2)\n",
    "    elif data['c1'][i] in only_r_list:\n",
    "        data['c1_book'][i] = 'only_ring'\n",
    "for i in range(len(data)):        \n",
    "    if data['c2'][i] in both_list:\n",
    "        data['c2_book'][i] = 'both'\n",
    "    if data['c2'][i] in only_h_list:\n",
    "        data['c2_book'][i] = 'only_hobbits'\n",
    "    if data['c2'][i] in only_r_list:\n",
    "        data['c2_book'][i] = 'only_ring'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=Graph('http://localhost:7474',user='',password='')\n",
    "g.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c02abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    item = data.iloc[i,:]\n",
    "    start_node=Node(item[5],  name=item[2], race=item[1])\n",
    "    end_node = Node(item[5],  name=item[3], race=item[0])\n",
    "    relation=Relationship(start_node,item[4],end_node)\n",
    "    g.merge(start_node,\"race\",\"name\")\n",
    "    g.merge(end_node, \"race\", \"name\")\n",
    "    g.merge(relation, \"race\", \"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f6d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    item = data.iloc[i,:]\n",
    "    start_node=Node(item[6], name=item[2], race=item[1])\n",
    "    end_node = Node(item[6], name=item[3], race=item[0])\n",
    "    relation=Relationship(start_node,item[4],end_node)\n",
    "    g.merge(start_node,\"book\",\"name\")\n",
    "    g.merge(end_node, \"book\", \"name\")\n",
    "    g.merge(relation, \"book\", \"name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
